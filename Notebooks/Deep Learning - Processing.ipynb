{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Deep Learning - Processing.ipynb","provenance":[],"collapsed_sections":["nR00SdqulV5S","VvAuMxbDlV5b"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"m3WpWEwwlV5K"},"source":["# Deep Learning - Processing"]},{"cell_type":"markdown","metadata":{"id":"qBLe_2hXlV5M"},"source":["## Importing Packages"]},{"cell_type":"code","metadata":{"id":"gIrLhCy0_9ji"},"source":["# try:\n","#     from google.colab import drive\n","#     drive.mount('/content/drive', force_remount=False)\n","# except:\n","#     pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6Jrok4nlV5T"},"source":["try:\n","    import google.colab\n","    IN_COLAB = True\n","    datasets_folder = '/content/drive/My Drive/Colab Notebooks/DataSets/'\n","    experiments_folder = '/content/drive/My Drive/Colab Notebooks/Experiments/'\n","except:\n","    IN_COLAB = False\n","    datasets_folder = '/Google Drive/Colab Notebooks/DataSets/'\n","    experiments_folder = '/Google Drive/Colab Notebooks/Experiments/'  \n","\n","print(\"In Colab:\", IN_COLAB)\n","print(\"Dataset Folder:\", datasets_folder)\n","print(\"Experiments Folder:\", experiments_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEH4jRqFsEcp"},"source":["!pip install livelossplot --quiet\n","!pip install -q -U keras-tuner --quiet\n","!pip install hiplot --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FkhkhXHlV5O"},"source":["import os\n","import gc\n","import json\n","import shutil\n","import time\n","import random\n","import IPython\n","from IPython.display import display\n","import hiplot as hip\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from sklearn.metrics import confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7kPsJqpX-GD"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping, Callback\n","from tensorflow.keras.layers import GlobalMaxPooling1D, AveragePooling1D , Input, Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, LSTM, GRU\n","from tensorflow.keras.utils import plot_model\n","from livelossplot import PlotLossesKerasTF\n","import kerastuner as kt\n","print(\"Using Tensorflow\", tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nR00SdqulV5S"},"source":["## Packages Parameters"]},{"cell_type":"code","metadata":{"id":"yek22t76LBNY"},"source":["# seed_value = 7\n","# np.random.seed(seed_value)\n","# tf.random.set_seed(seed_value)\n","# random.seed(seed_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVKdK7nMyDJ2"},"source":["plt.rcParams[\"figure.figsize\"] = (16,6)\n","pd.options.display.float_format = '{:.5f}'.format"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYpqWHmElV5X"},"source":["## Dataset Functions"]},{"cell_type":"code","metadata":{"id":"EBC9M9TUxpY8"},"source":["moving_window=False\n","last_label=False\n","data_class_labels = [\"irregular_speed_bump\", \"regular_speed_bump\"]\n","join_labels=True\n","join_labels_name=\"speed_bump\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMcwk8WrlV5Y"},"source":["def getDataSets(folder=datasets_folder):\n","    \n","    \"\"\"Load raw datasets from the disk.\n","\n","    Args:\n","        folder (str): Root folder of PVS datasets. Within this root folder are the PVS folders and their files.\n","\n","    Returns:\n","        dict: datasets in a dict form: \n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    datasets = {}\n","    \n","    for i in range(1, 10):\n","        \n","        dataset_folder = os.path.join(folder, \"PVS \" + str(i))\n","\n","        left =   pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_left.csv'))              #,  float_precision=\"high\" , dtype=np.float32\n","        right =  pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_right.csv'))             #,  float_precision=\"high\" , dtype=np.float32\n","        labels = pd.read_csv(os.path.join(dataset_folder, 'dataset_labels.csv'), dtype=np.uint8)    #,  float_precision=\"high\"\n","        \n","        datasets[\"pvs_\" + str(i)] = {\n","            \"left\": left,\n","            \"right\": right,\n","            \"labels\": labels\n","        }\n","    \n","    return datasets\n","\n","def getFields(acc=False, gyro=False, mag=False, temp=False, speed=False, location=False, below_suspension=False, above_suspension=False, dashboard=False):\n","    \n","    \"\"\"Get fields names filtering by data type and placement.\n","\n","    Args:\n","        acc (bool): to return accelerometer fields.\n","            (default is False)\n","        gyro (bool): to return gyroscope fields.\n","            (default is False)\n","        mag (bool): to return magnetometer fields.\n","            (default is False)\n","        temp (bool): to return temperature field.\n","            (default is False)\n","        speed (bool): to return speed field.\n","            (default is False)\n","        location (bool): to return GPS location fields.\n","            (default is False)\n","        below_suspension (bool): to return fields of data sampled next and below suspension.\n","            (default is False)\n","        above_suspension (bool): to return fields of data sampled next and above suspension.\n","            (default is False)\n","        dashboard (bool): to return fields of data sampled in the dashboard.\n","            (default is False)\n","\n","    Returns:\n","        list: list of fields.\n","    \"\"\"\n","\n","    all_fields = [\n","        'timestamp', \n","        'acc_x_dashboard', 'acc_y_dashboard', 'acc_z_dashboard',\n","        'acc_x_above_suspension', 'acc_y_above_suspension', 'acc_z_above_suspension', \n","        'acc_x_below_suspension', 'acc_y_below_suspension', 'acc_z_below_suspension', \n","        'gyro_x_dashboard', 'gyro_y_dashboard', 'gyro_z_dashboard', \n","        'gyro_x_above_suspension', 'gyro_y_above_suspension', 'gyro_z_above_suspension',\n","        'gyro_x_below_suspension', 'gyro_y_below_suspension', 'gyro_z_below_suspension', \n","        'mag_x_dashboard', 'mag_y_dashboard', 'mag_z_dashboard', \n","        'mag_x_above_suspension', 'mag_y_above_suspension', 'mag_z_above_suspension', \n","        'temp_dashboard', 'temp_above_suspension', 'temp_below_suspension', \n","        'timestamp_gps', 'latitude', 'longitude', 'speed'\n","    ]\n","    \n","    return_fields = []\n","    \n","    for field in all_fields:\n","            \n","        data_type = False\n","        placement = False\n","        \n","        if (speed and field == \"speed\"):\n","            placement = data_type = True\n","            \n","        if (location and (field == \"latitude\" or field == \"longitude\")):\n","            placement = data_type = True\n","        \n","        if (acc):\n","            data_type = data_type or field.startswith(\"acc_\")\n","        \n","        if (gyro):\n","            data_type = data_type or field.startswith(\"gyro_\")\n","            \n","        if (mag):\n","            data_type = data_type or field.startswith(\"mag_\")\n","            \n","        if (temp):\n","            data_type = data_type or field.startswith(\"temp_\")\n","            \n","        if (below_suspension):\n","            placement = placement or field.endswith(\"_below_suspension\")\n","            \n","        if (above_suspension):\n","            placement = placement or field.endswith(\"_above_suspension\")\n","            \n","        if (dashboard):\n","            placement = placement or field.endswith(\"_dashboard\")\n","        \n","        if (data_type and placement):\n","            return_fields.append(field)\n","            \n","    return return_fields\n","\n","def getSubSets(datasets, fields, labels=data_class_labels, join_labels=join_labels, join_labels_name=join_labels_name):\n","\n","    \"\"\"Get subsets from raw datasets. For each PVS dataset, extract a subset with only fields/labels passed.\n","\n","    Args:\n","        datasets (dict): raw PVS datasets.\n","        fields (string[]): fields to extract.\n","        labels (string[]): labels to extract.\n","\n","    Returns:\n","        dict: subsets in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    subsets = {}\n","    \n","    for key in datasets.keys():\n","        \n","        subsets[key] = {\n","            \"left\": datasets[key][\"left\"][fields],\n","            \"right\": datasets[key][\"right\"][fields],\n","            \"labels\": datasets[key][\"labels\"][labels]\n","        }\n","\n","        if join_labels:\n","            subsets[key][\"labels\"] = pd.DataFrame(data=subsets[key][\"labels\"].sum(axis=1), columns=[join_labels_name])\n","    \n","    return subsets\n","\n","def getNormalizedDataMinMax(subsets, scaler_range=(-1,1)):\n","\n","    \"\"\"Get normalized data. Uses MinMaxScaler.\n","\n","    Args:\n","        subsets (dict): subsets to be normalized.\n","        scaler_range (tuple): range to scale, such as (0,1) or (-1,1).\n","\n","    Returns:\n","        dict: subsets normalized in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    scaler = MinMaxScaler(feature_range=scaler_range)\n","    return getNormalizedData(subsets, scaler)\n","\n","def getNormalizedDataRobust(subsets): \n","\n","    \"\"\"Get standardized data. Uses RobustScaler.\n","\n","    Args:\n","        subsets (dict): subsets to be standardized.\n","\n","    Returns:\n","        dict: subsets normalized in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    scaler = RobustScaler()            \n","    return getNormalizedData(subsets, scaler)\n","\n","def getNormalizedData(subsets, scaler):\n","    \n","    \"\"\"Get standardized/normalized data.\n","\n","    Args:\n","        subsets (dict): subsets to be standardized/normalized.\n","        scaler (object): scaler to transform values.\n","\n","    Returns:\n","        dict: subsets normalized in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","    \n","    normalized_sets = {}\n","    learn_data = pd.DataFrame()\n","\n","    for pvs in subsets.keys():\n","        for side in [\"left\", \"right\"]:\n","            learn_data = learn_data.append(subsets[pvs][side], ignore_index=True)\n","\n","    scaler = scaler.fit(learn_data)\n","    del learn_data\n","    \n","    for pvs in subsets.keys():\n","        \n","        normalized_sets[pvs] = {\n","            'left':  pd.DataFrame(data=scaler.transform(subsets[pvs]['left']),  columns=subsets[pvs]['left'].columns),\n","            'right': pd.DataFrame(data=scaler.transform(subsets[pvs]['right']), columns=subsets[pvs]['right'].columns),\n","            'labels': subsets[pvs]['labels']\n","        }\n","                    \n","    return normalized_sets \n","\n","def getReshapedData(subsets, shape):  \n","\n","    \"\"\"Reshape data.\n","\n","    Args:\n","        subsets (dict): subsets to be reshaped.\n","        shape (tuple): shape to reshape data. Must have the form (..., ..., ..., features).\n","        last_label (boolean): if use last label in the window, otherwise uses mode label.\n","\n","    Returns:\n","        dict: subsets reshaped in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": np.array, \n","                \"right\": np.array, \n","                \"labels\": np.array \n","            } \n","        }\n","    \"\"\"\n","\n","    shape = tuple([x for x in shape if x is not None])\n","\n","    if last_label:\n","        print(\"Using last label\")\n","    else:\n","        print(\"Using mode label\")\n","\n","    reshaped_sets = {}\n","\n","    window = 1\n","\n","    for dim in shape:\n","        window = window * dim\n","\n","    window = int(window / shape[-1])\n","\n","    for key in subsets.keys():\n","\n","        reshaped_sets[key] = {};\n","\n","        for side in ['left', 'right']:\n","\n","            inputs = subsets[key][side].values\n","            outputs = subsets[key][\"labels\"].values\n","  \n","            inputs_reshaped = []\n","            outputs_reshaped = []\n","\n","            for i in range(window, len(inputs) + 1):\n","                \n","                input_window = inputs[i-window:i, :]\n","\n","                if last_label:\n","                    output_window = outputs[i-1, :]\n","                else: # mode label\n","                    output_window = outputs[i-window:i, :].mean(axis=0).round(0)\n","\n","                if moving_window or i % window == 0 or output_window[0] > 0:\n","                    inputs_reshaped.append(input_window.reshape(shape))\n","                    outputs_reshaped.append(output_window) \n","\n","            reshaped_sets[key][side] = np.array(inputs_reshaped) # inputs_reshaped\n","            reshaped_sets[key]['labels'] = np.array(outputs_reshaped) # outputs_reshaped\n","            del inputs_reshaped, outputs_reshaped\n","\n","    return reshaped_sets, window\n","\n","def getTrainValidationSets(reshaped_sets, sets_train, sets_test, sides_train, sides_test):\n","\n","    \"\"\"Get train and test sets from pre-processed sets.\n","\n","    Args:\n","        reshaped_sets (dict): all pre-processed sets.\n","        sets_train (string[]): PVS datasets to be used in train.\n","        sets_test (string[]): PVS datasets to be used in validation.\n","        sides_train (string[]): PVS datasets sides to be used in train. \n","        sides_test (string[]): PVS datasets sides to be used in validation. \n","\n","    Returns:\n","        input_train (list|np.array): input train values.\n","        input_validation (list|np.array): input validation values. \n","        output_train (list|np.array): output train values.\n","        output_validation (list|np.array): output validation values. \n","    \"\"\"\n","\n","    input_train = []\n","    input_validation = []\n","    output_train = []\n","    output_validation = []\n","\n","    for key in reshaped_sets.keys():\n","\n","        for side in [\"left\", \"right\"]:\n","\n","            input_ref = None\n","            output_ref = None\n","                \n","            if (key in sets_train and side in sides_train):\n","                input_ref = input_train\n","                output_ref = output_train\n","            \n","            elif (key in sets_test and side in sides_test):\n","                input_ref = input_validation\n","                output_ref = output_validation\n","            \n","            inputs = reshaped_sets[key][side]\n","            outputs = reshaped_sets[key][\"labels\"]\n","\n","            if input_ref is not None:\n","                for inp in inputs:\n","                    input_ref.append(inp)\n","\n","            if output_ref is not None:\n","                for out in outputs:\n","                    output_ref.append(out)   \n","\n","    return np.array(input_train), np.array(input_validation), np.array(output_train), np.array(output_validation) # input_train, input_validation, output_train, output_validation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gffYgpM44Xwq"},"source":["## Experiments Parameters"]},{"cell_type":"code","metadata":{"id":"o8TDC5VdlV5i"},"source":["experiment_by_dataset = [\n","    { \n","        \"sets_train\": [\"pvs_1\", \"pvs_3\", \"pvs_4\", \"pvs_6\", \"pvs_7\", \"pvs_9\"], \n","        \"sets_test\": [\"pvs_2\", \"pvs_5\", \"pvs_8\"],\n","        \"sides_train\": [\"left\", \"right\"],\n","        \"sides_test\": [\"left\", \"right\"]\n","    },\n","    { \n","        \"sets_train\": [\"pvs_1\", \"pvs_2\", \"pvs_3\", \"pvs_7\", \"pvs_8\", \"pvs_9\"], \n","        \"sets_test\":  [\"pvs_4\", \"pvs_5\", \"pvs_6\"],\n","        \"sides_train\": [\"left\", \"right\"],\n","        \"sides_test\": [\"left\", \"right\"]\n","    },\n","    { \n","        \"sets_train\": [\"pvs_1\", \"pvs_2\", \"pvs_4\", \"pvs_6\", \"pvs_8\", \"pvs_9\"], \n","        \"sets_test\":  [\"pvs_3\", \"pvs_5\", \"pvs_7\"],\n","        \"sides_train\": [\"left\", \"right\"],\n","        \"sides_test\": [\"left\", \"right\"]\n","    },\n","    # { \n","    #     \"sets_train\": [\"pvs_1\", \"pvs_2\", \"pvs_3\", \"pvs_4\", \"pvs_5\", \"pvs_6\", \"pvs_7\", \"pvs_8\", \"pvs_9\"], \n","    #     \"sets_test\":  [\"pvs_1\", \"pvs_2\", \"pvs_3\", \"pvs_4\", \"pvs_5\", \"pvs_6\", \"pvs_7\", \"pvs_8\", \"pvs_9\"],\n","    #     \"sides_train\": [\"left\"],\n","    #     \"sides_test\": [\"right\"]\n","    # }\n","]\n","\n","experiment_by_placement = [\n","    (\"Below Suspension\", getFields(acc=True, gyro=True, speed=True, below_suspension=True)),\n","    (\"Above Suspension\", getFields(acc=True, gyro=True, speed=True, above_suspension=True)),\n","    (\"Dashboard\",        getFields(acc=True, gyro=True, speed=True, dashboard=True))\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvAuMxbDlV5b"},"source":["## Model Management"]},{"cell_type":"code","metadata":{"id":"S0x7ZzLF17FO"},"source":["batch_size=64 # 32\n","epochs=1000\n","patience=10\n","min_delta=0.0001\n","shuffle=True\n","num_tests=2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xYvBDqR1tjj"},"source":["class ClearTrainingOutput(Callback):\n","        \n","    def on_train_end(*args, **kwargs):\n","        IPython.display.clear_output(wait = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WU14cNYZlV5c"},"source":["def createPathIfNotExists(path):\n","\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def modelFileSavedFormat(filename):\n","    return filename + '-train-acc-{acc:.5f}-val-acc-{val_acc:.5f}.hdf5'\n","\n","def saveModelDiagram(model, path, filename, show=True):\n","    \n","    createPathIfNotExists(path)\n","    plot_model(model, to_file=os.path.join(path, filename + '.png'), show_shapes=True, show_layer_names=True)\n","\n","    if show:\n","        display(plot_model(model, show_shapes=True, show_layer_names=True))\n","        display(model.summary())\n","    \n","def showHistory(history):\n","    \n","    for key in history.history.keys():\n","        plt.plot(history.history[key], label=key)\n","    \n","    plt.legend()\n","    \n","def fitModel(model, input_train, output_train, input_validation, output_validation, experiment_path, file_preffix):\n","    \n","    createPathIfNotExists(experiment_path)\n","\n","    # logger_file = os.path.join(experiment_path, file_preffix + '-training-log.csv')\n","    # csv_logger = CSVLogger(logger_file, append=True)\n","\n","    checkpoint_validation_file = os.path.join(experiment_path, modelFileSavedFormat(file_preffix))\n","    checkpoint_validation = ModelCheckpoint(filepath=checkpoint_validation_file, save_best_only=True, monitor='val_acc', mode='max') # verbose=1\n","    \n","    early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=patience, min_delta=min_delta, restore_best_weights=True, verbose=1)\n","\n","    callbacks=[PlotLossesKerasTF(), checkpoint_validation, early_stopping, ClearTrainingOutput()] # csv_logger\n","\n","    return model.fit(\n","        input_train, output_train, validation_data=(input_validation, output_validation), \n","        epochs=epochs, batch_size=batch_size, validation_batch_size=batch_size, \n","        callbacks=callbacks, shuffle=shuffle, use_multiprocessing=True, workers=16, verbose=0) # verbose=1\n","\n","def predictModel(model, inputs):\n","    return model.predict(inputs)\n","    \n","def evaluateModel(model, inputs, outputs):\n","    return model.evaluate(inputs, outputs, batch_size=batch_size, verbose=0)\n","    \n","def loadWeights(model, file_path):\n","    model.load_weights(file_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v587XcMKAzSp"},"source":["def tuneModel(tuneModel, modelParameters, experiment_by_placement_selected, experiment_by_dataset_selected, input_shape, output_shape):\n","    \n","    placement, fields = experiment_by_placement_selected\n","    subsets = getSubSets(datasets.copy(), fields, data_class_labels)\n","    normalized_sets = getNormalizedDataRobust(subsets)\n","    del subsets\n","    reshaped_sets, window_size = getReshapedData(normalized_sets.copy(), input_shape)\n","    del normalized_sets\n","    sets_train = experiment_by_dataset_selected['sets_train']\n","    sets_test = experiment_by_dataset_selected['sets_test']\n","    sides_train = experiment_by_dataset_selected['sides_train']\n","    sides_test = experiment_by_dataset_selected['sides_test']\n","    input_train, input_validation, output_train, output_validation = getTrainValidationSets(reshaped_sets, sets_train, sets_test, sides_train, sides_test)\n","    del reshaped_sets\n","\n","    print(\"Input Train Shape:\", input_train.shape, \"Output Train Shape:\", output_train.shape)\n","    print(\"Input Validation Shape:\", input_validation.shape, \"Output Validation Shape:\", output_validation.shape)\n","\n","    model_args = modelParameters(input_shape, output_shape)\n","    model, model_name = tuneModel(**model_args)\n","    experiment_folder = os.path.join(experiments_folder, model_name)\n","\n","    tuner = kt.Hyperband(\n","        model,\n","        objective='val_acc',\n","        max_epochs=10,\n","        factor=3,\n","        directory=experiment_folder\n","    )\n","\n","    early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10)\n","    clear_output = ClearTrainingOutput()\n","\n","    params = {\n","        \"x\": input_train,\n","        \"y\": output_train, \n","        \"validation_data\":(input_validation, output_validation),\n","        \"epochs\": 10,\n","        \"callbacks\":[early_stopping, clear_output]\n","    }\n","\n","    tuner.search_space_summary()\n","\n","    return tuner, params\n","\n","def getTunerBestResults(tuner, num_trials=1):\n","    tuner.results_summary(num_trials=num_trials)\n","    # best_model = tuner.get_best_models(num_models=1)\n","    return tuner.get_best_hyperparameters(num_trials=num_trials)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2vDUTUC3LbT"},"source":["## Training Functions"]},{"cell_type":"code","metadata":{"id":"hIw1Rq3Z9hnC"},"source":["def saveExecutionLog(path, data, columns=['placement', 'experiment', \"window\", \"scaler\", \"input_shape\", \"output_shape\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"]):\n","    \n","    \"\"\" Save a log for each experiment execution (params for each execution)\n","    \"\"\"\n","    \n","    save = pd.DataFrame(data=data, columns=columns)\n","    save.to_csv(os.path.join(path, \"experiment-execution-log.csv\"), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZ_h99QuxnuW"},"source":["def manageFiles(history, experiment_folder, experiment_file):\n","\n","    \"\"\" Retrieve best model from num_tests executions\n","    \"\"\"\n","\n","    test = -1\n","    index = -1\n","    val_acc = -1\n","    \n","    for i in range(0, num_tests):\n","\n","        max_value = max(history[i]['val_acc'])\n","\n","        if max_value > val_acc:\n","            val_acc = max_value\n","            test = i\n","            index = history[i]['val_acc'].index(max_value)\n","            \n","    train_acc = history[test]['acc'][index]\n","    train_loss = history[test]['loss'][index]\n","    val_acc = history[test]['val_acc'][index]\n","    val_loss = history[test]['val_loss'][index]\n","    \n","    test_folder = os.path.join(experiment_folder, \"Test \" + str(test + 1)) \n","    file = modelFileSavedFormat(experiment_file).format(**{'acc': train_acc, 'val_acc': val_acc})\n","\n","    move_from = os.path.join(test_folder, file)\n","    move_to = os.path.join(experiment_folder, file)\n","\n","    shutil.move(move_from, move_to)\n","    \n","    for i in range(0,num_tests):\n","        shutil.rmtree(os.path.join(experiment_folder, \"Test \" + str(i + 1)))\n","\n","    return [train_loss, val_loss, train_acc, val_acc]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ppZlIZI3eLz"},"source":["def getLoadBar():\n","\n","    \"\"\" Load bar for experiments progress\n","    \"\"\"\n","    \n","    global load_bar_placement, load_bar_dataset, load_bar_input_shape, load_bar_retries\n","    \n","    experiment_total_placement = len(experiment_by_placement)\n","    experiment_total_dataset = len(experiment_by_dataset)\n","    experiment_total_input_shapes = len(input_shapes)\n","    \n","    load_bar_placement = tqdm(total=experiment_total_placement, desc='Experiment By Placement')\n","    load_bar_dataset = tqdm(total=experiment_total_dataset, desc='Experiment By Dataset')\n","    load_bar_input_shape = tqdm(total=experiment_total_input_shapes, desc='Experiment By Input Shape')\n","    load_bar_retries = tqdm(total=num_tests, desc='Retries')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJS-OGcs4R9O"},"source":["def run(modelBuilder, modelParameters):\n","\n","    load_bar_placement.reset()\n","\n","    for placement, fields in experiment_by_placement:\n","\n","        load_bar_dataset.reset()\n","        subsets = getSubSets(datasets.copy(), fields, data_class_labels)\n","        normalized_sets = getNormalizedDataRobust(subsets) #getNormalizedDataMinMax(subsets, (-1,1))\n","        \n","        # Clean Memory\n","        del subsets\n","        gc.collect()\n","\n","        for experiment_number in range(0, len(experiment_by_dataset)):\n","\n","            load_bar_input_shape.reset()\n","            sets_train = experiment_by_dataset[experiment_number]['sets_train']\n","            sets_test = experiment_by_dataset[experiment_number]['sets_test']\n","            sides_train = experiment_by_dataset[experiment_number]['sides_train']\n","            sides_test = experiment_by_dataset[experiment_number]['sides_test']\n","            execution_log = []\n","\n","            for input_shape in input_shapes:\n","\n","                load_bar_retries.reset()\n","                history = []\n","                model_args = modelParameters(input_shape, output_shape)\n","\n","                reshaped_sets, window_size = getReshapedData(normalized_sets.copy(), input_shape)\n","                input_train, input_validation, output_train, output_validation = getTrainValidationSets(reshaped_sets, sets_train, sets_test, sides_train, sides_test)\n","                \n","                # Clean Memory\n","                del reshaped_sets\n","                gc.collect()\n","\n","                print(\"Input Train Shape:\", input_train.shape, \"Output Train Shape:\", output_train.shape)\n","                print(\"Input Test Shape:\", input_validation.shape, \"Output Test Shape:\", output_validation.shape)\n","\n","                for test in range(0, num_tests):\n","\n","                    model, model_name = modelBuilder(**model_args)\n","                    experiment_folder = os.path.join(experiments_folder, model_name, placement, \"Dataset Experiment \" + str(experiment_number + 1))\n","                    test_folder = os.path.join(experiment_folder, \"Test \" + str(test + 1))\n","                    diagram_file = \"dataset-experiment-\" + str(experiment_number + 1) + \"-window-\" + str(window_size)\n","                    experiment_file = diagram_file + \"-robust-scaler\"\n","                    saveModelDiagram(model, experiment_folder, diagram_file)\n","                    fit_history = fitModel(model, input_train, output_train, input_validation, output_validation, test_folder, experiment_file)\n","                    history.append(fit_history.history)\n","                    load_bar_retries.update(1)\n","\n","                    # Clean Memory\n","                    del model, fit_history\n","                    gc.collect()\n","\n","                metrics = manageFiles(history, experiment_folder, experiment_file)\n","                \n","                execution_log.append([\n","                    placement,\n","                    experiment_number + 1, \n","                    window_size, \n","                    \"Robust Scaler\", \n","                    str(input_shape), \n","                    str(output_shape)\n","                ] + metrics)\n","\n","                saveExecutionLog(experiment_folder, execution_log)\n","                load_bar_input_shape.update(1)\n","\n","                # Clean Memory\n","                del history, input_train, input_validation, output_train, output_validation, metrics\n","                gc.collect()\n","                time.sleep(5)\n","\n","            load_bar_dataset.update(1)       \n","        \n","        # Clean Memory\n","        del normalized_sets\n","        load_bar_placement.update(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Boeyib9csLVj"},"source":["## Result Analysis"]},{"cell_type":"code","metadata":{"id":"p8Sp6E6bhYti"},"source":["data_class_labels_plot = ['NSB', 'SB']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"52G9SKPWgkZh"},"source":["def generateMetrics(modelBuilder, modelParameters):\n","\n","    experiment_total_placement = len(experiment_by_placement)\n","    experiment_total_dataset = len(experiment_by_dataset)\n","    experiment_total_input_shapes = len(input_shapes)\n","    \n","    load_bar_placement = tqdm(total=experiment_total_placement, desc='Experiment By Placement')\n","    load_bar_dataset = tqdm(total=experiment_total_dataset, desc='Experiment By Dataset')\n","    load_bar_input_shape = tqdm(total=experiment_total_input_shapes, desc='Experiment By Input Shape')\n","\n","    load_bar_placement.reset()\n","\n","    for placement, fields in experiment_by_placement:\n","\n","        load_bar_dataset.reset()\n","        subsets = getSubSets(datasets.copy(), fields, data_class_labels)\n","        normalized_sets = getNormalizedDataRobust(subsets) #getNormalizedDataMinMax(subsets, (-1,1))\n","        \n","        # Clean Memory\n","        del subsets\n","        gc.collect()\n","\n","        for experiment_number in range(0, len(experiment_by_dataset)):\n","\n","            load_bar_input_shape.reset()\n","            sets_train = experiment_by_dataset[experiment_number]['sets_train']\n","            sets_test = experiment_by_dataset[experiment_number]['sets_test']\n","            sides_train = experiment_by_dataset[experiment_number]['sides_train']\n","            sides_test = experiment_by_dataset[experiment_number]['sides_test']\n","            experiments_report = []\n","\n","            for input_shape in input_shapes:\n","\n","                reshaped_sets, window_size = getReshapedData(normalized_sets.copy(), input_shape)\n","                input_train, input_validation, output_train, output_validation = getTrainValidationSets(reshaped_sets, sets_train, sets_test, sides_train, sides_test)\n","                \n","                # Clean Memory\n","                del reshaped_sets\n","                gc.collect()\n","\n","                model_args = modelParameters(input_shape, output_shape)\n","                model, model_name = modelBuilder(**model_args)\n","                experiment_folder = os.path.join(experiments_folder, model_name, placement, \"Dataset Experiment \" + str(experiment_number + 1))\n","                experiment_file = \"dataset-experiment-\" + str(experiment_number + 1) + \"-window-\" + str(window_size) + \"-robust-scaler\"\n","\n","                for file in os.listdir(experiment_folder):\n","                    if file.startswith(experiment_file):\n","                        experiment_file = file\n","\n","                loadWeights(model, os.path.join(experiment_folder, experiment_file))\n","                predictions = predictModel(model, input_validation)\n","\n","                experiments_report.append({\n","                    'params': {\n","                        'file': experiment_file,\n","                        'placement': placement,\n","                        'experiment_number': experiment_number + 1, \n","                        'window_size': window_size,\n","                        'scaler': 'Robust Scaler',\n","                        'input_shape': str(input_shape),\n","                        'output_shape': str(output_shape)\n","                    },\n","                    'confusion_matrix': confusion_matrix(output_validation.round(0), predictions.round(0)).tolist(),\n","                    'classification_report': classification_report(output_validation.round(0), predictions.round(0), target_names=data_class_labels_plot, output_dict=True)\n","                })\n","\n","                with open(os.path.join(experiment_folder, 'classification_report.json'), 'w') as fp:\n","                    json.dump(experiments_report, fp, indent=4)\n","\n","                load_bar_input_shape.update(1)\n","\n","                # Clean Memory\n","                del model, predictions, input_train, input_validation, output_train, output_validation\n","                gc.collect()\n","                time.sleep(5)\n","\n","            load_bar_dataset.update(1)       \n","        \n","        # Clean Memory\n","        del normalized_sets\n","        load_bar_placement.update(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"calUL5P5g3or"},"source":["def plotConfusionMatrix(values, title):\n","\n","    \"\"\" Plot confusion matrix\n","    \"\"\"\n","\n","    con_mat_df = pd.DataFrame(values, index=data_class_labels_plot, columns=data_class_labels_plot)\n","    figure = plt.figure(figsize=(4,4))\n","    sns.set(font_scale=1.2)\n","    sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues, annot_kws={\"size\": 14})\n","    plt.tight_layout()\n","    plt.title(title)\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","    figure.savefig('confusion_matrix.png', bbox_inches=\"tight\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9TUEwohlV5m"},"source":["def plotHighDimensionalData(data):\n","    hip.Experiment.from_dataframe(data).display()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKBy2HlNNfu4"},"source":["def getInfoData(model_name):\n","\n","    placement_info = {}\n","        \n","    for placement, fields in experiment_by_placement:\n","\n","        dataset_data = {}\n","        \n","        for experiment_number in range(0, len(experiment_by_dataset)):\n","\n","            folder = os.path.join(experiments_folder, model_name, placement, \"Dataset Experiment \" + str(experiment_number + 1))\n","            log_file = os.path.join(folder, 'experiment-execution-log.csv')\n","            metrics_file = os.path.join(folder, 'classification_report.json')\n","\n","            dataset_experiment_name = 'dataset_experiment' + str(experiment_number + 1)\n","            dataset_data[dataset_experiment_name] = {}\n","            dataset_data[dataset_experiment_name]['logs'] = pd.read_csv(log_file)\n","\n","            with open(metrics_file) as f:\n","                dataset_data[dataset_experiment_name]['metrics'] = json.load(f)\n","\n","        placement_info[placement] = dataset_data\n","\n","    return placement_info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1Q5vCtDNkiN"},"source":["def getInfoDataParsed(model_name):\n","\n","    placement_info = {}\n","        \n","    for placement, fields in experiment_by_placement:\n","\n","        dataset_info = {}\n","        average = None\n","        \n","        for experiment_number in range(0, len(experiment_by_dataset)):\n","\n","            folder = os.path.join(experiments_folder, model_name, placement, \"Dataset Experiment \" + str(experiment_number + 1))\n","            log_file = os.path.join(folder, 'experiment-execution-log.csv')\n","            metrics_file = os.path.join(folder, 'classification_report.json')\n","\n","            logs_content = pd.read_csv(log_file)\n","\n","            with open(metrics_file) as f:\n","                metrics_content = json.load(f)\n","\n","            window_info = []\n","            \n","            for window in [100, 200, 300, 400, 500]:\n","\n","                log_row = logs_content.loc[logs_content['window'] == window]\n","\n","                metrics_row = None\n","\n","                for row in metrics_content:\n","                    if row['params']['window_size'] == window:\n","                        metrics_row = row['classification_report']\n","                        break\n","\n","                window_info.append({\n","                    'Window': log_row['window'].values[0],\n","                    'Train Loss': log_row['train_loss'].values[0],\n","                    'Val Loss': log_row['val_loss'].values[0],\n","                    'Train Acc': log_row['train_acc'].values[0],\n","                    'Val Acc': log_row['val_acc'].values[0],\n","                    'Precision - SP': metrics_row['With Speed Bump']['precision'],\n","                    'Precision - NSP': metrics_row['Without Speed Bump']['precision'],\n","                    'Recall - SP': metrics_row['With Speed Bump']['recall'],\n","                    'Recall - NSP': metrics_row['Without Speed Bump']['recall'],\n","                    'F1-Score - SP': metrics_row['With Speed Bump']['f1-score'],\n","                    'F1-Score - NSP': metrics_row['Without Speed Bump']['f1-score']\n","                })\n","\n","            if average is None:\n","                average = pd.DataFrame.from_dict(window_info)\n","            else:\n","                average += pd.DataFrame.from_dict(window_info)\n","            \n","            dataset_info[\"Dataset Experiment \" + str(experiment_number + 1)] = pd.DataFrame.from_dict(window_info)\n","            \n","        dataset_info[\"Dataset Average\"] = average/3\n","        placement_info[placement] = dataset_info\n","\n","    return placement_info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qk56Q2gT0l6c"},"source":["print(\"V37\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vN6trXVr9kF9"},"source":[""],"execution_count":null,"outputs":[]}]}